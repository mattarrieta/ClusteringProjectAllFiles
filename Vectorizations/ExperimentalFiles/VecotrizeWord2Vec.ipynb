{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib.pyplot import text\n",
    "from numpy import vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import linalg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_l = []\n",
    "\n",
    "for root, dirs, files in os.walk('IdealFolders'):\n",
    "    for filename in files:\n",
    "        file_path_l.append(os.path.join(root, filename))\n",
    "\n",
    "text_l = []\n",
    "for file_path in file_path_l:\n",
    "    filey = open(file_path,encoding=\"mbcs\")\n",
    "    text_l.append(filey.read())\n",
    "    filey.close()\n",
    "print(text_l[-1])\n",
    "types_l = []\n",
    "for namey in file_path_l:\n",
    "    splitted = namey.split('\\\\')\n",
    "    types_l.append(splitted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load('text8')\n",
    "#dataset = api.load(\"wiki-english-20171001\")\n",
    "data = [d for d in dataset]\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5030436  -2.3145452  -0.9601538   0.84593093  1.9068558  -1.6672593\n",
      " -0.16274473 -0.8554145  -1.5541695  -0.44815925  0.6256429   2.3774815\n",
      " -1.0410227  -0.06683082 -0.5898638  -0.58474135 -0.84530634  2.4935546\n",
      " -3.104144    1.1105659  -1.7697549   2.4041336   1.8496189   0.7126777\n",
      " -0.2421457  -2.4694948  -1.2197285  -2.6305592   1.2461345  -0.02726633\n",
      " -1.1016437  -2.04939    -2.3547075  -5.2624903  -1.9525527  -1.7742271\n",
      "  0.68781394 -5.0592446  -0.76815605 -1.9250462 ]\n",
      "0.5102303\n"
     ]
    }
   ],
   "source": [
    "def cosine(vector1,vector2):\n",
    "    cosV12 = np.dot(vector1, vector2) / (linalg.norm(vector1) * linalg.norm(vector2))\n",
    "    return cosV12\n",
    "\n",
    "dataset = text_l\n",
    "List_of_List_of_words = []\n",
    "for x in range(len(dataset)):\n",
    "   List_of_List_of_words.append(dataset[x].split())\n",
    "print(model.infer_vector(List_of_List_of_words[0]))\n",
    "vector1 = model.infer_vector(List_of_List_of_words[0])\n",
    "vector2 = model.infer_vector(List_of_List_of_words[11])\n",
    "vector3 = model.infer_vector(List_of_List_of_words[-1])\n",
    "print(cosine(vector1,vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = text_l\n",
    "List_of_List_of_words = []\n",
    "for x in range(len(dataset)):\n",
    "   List_of_List_of_words.append(dataset[x].split())\n",
    "data = [d for d in dataset]\n",
    "#print((List_of_List_of_words[-1]))\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(List_of_List_of_words))\n",
    "print(List_of_List_of_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "ivec = model.infer_vector(['School', 'Department', 'Compartment'])\n",
    "print(ivec)\n",
    "print(model.dv.most_similar(positive=[ivec], topn=10))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4bec7b648d61350689a0ccb93b53155b5d49d036bbaf30ce84901e3a87cea97"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
