{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import os, itertools\n",
    "import warnings\n",
    "from matplotlib.pyplot import text\n",
    "from numpy import vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statistics\n",
    "from nltk.corpus import words\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "import math\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from JaccardIndexUpdated import JaccardIndex, ClustersData, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def drop_duplicates(labeled_df):\n",
    "    data_copy_df = labeled_df.copy()\n",
    "    data_no_duplicates = data_copy_df.drop_duplicates(subset = ['text'])\n",
    "    return data_no_duplicates\n",
    "\n",
    "def crop_documents(labeled_df, document_cutoff_length):\n",
    "    '''Returns a df with 'cropped_documents' columns'''\n",
    "    labeled_df_copy = labeled_df.copy()\n",
    "    labeled_df_copy['tokens_cropped'] = labeled_df_copy['text'].apply(lambda x: x.split()[:document_cutoff_length])\n",
    "    labeled_df_copy['cropped_documents'] = labeled_df_copy['tokens_cropped'].apply(lambda x: ' '.join(x))\n",
    "    labeled_df_copy = labeled_df_copy.drop(columns = ['tokens_cropped'])\n",
    "    return labeled_df_copy\n",
    "\n",
    "def get_corpus(labeled_df, on_entire_doc, document_cut_off):\n",
    "    if on_entire_doc == False:\n",
    "        crop_documents_df = crop_documents(labeled_df, document_cut_off)\n",
    "        corpus_to_train_on = crop_documents_df['cropped_documents'].to_list()\n",
    "    else:\n",
    "        corpus_to_train_on = labeled_df['text'].to_list()\n",
    "    return corpus_to_train_on\n",
    "\n",
    "def vectorize_count_vectorizer(labeled_df, train_on_entire_doc, vectorize_entire_doc, document_cut_off, max_df, min_df, ngram_range, binary= False):\n",
    "    '''train_on_entire_doc = True --> determine vocabulary and stop words using entire document, else use document_cut_off\n",
    "    vectorize_on_entire_doc = True --> vectorize entire doc, otherwise only vectorize cropped document\n",
    "    document_cut_off = N --> used to crop documents to N words\n",
    "    max_df: any words appearing with frequence > max_df will be excluded\n",
    "    min_df: any words appearing with frequency < min_df will be excluded\n",
    "    ngram_range: us ngrams to create vocabulary and vectorize\n",
    "    binary = True, vecorizes documents using 1 or 0, vs word count\n",
    "    '''\n",
    "    no_dups_df = drop_duplicates(labeled_df)\n",
    "    training_corpus = get_corpus(no_dups_df, train_on_entire_doc, document_cut_off)\n",
    "    vectorizing_corpus = get_corpus(labeled_df, vectorize_entire_doc, document_cut_off)\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range, min_df= min_df, max_df = max_df, binary = binary)\n",
    "    vectorizer.fit(training_corpus)\n",
    "    X = vectorizer.transform(vectorizing_corpus)\n",
    "    #X = vectorizer.fit_transform(vectorizing_corpus)\n",
    "    df = pd.DataFrame(X.todense(), index = labeled_df.index, columns = vectorizer.get_feature_names())\n",
    "    return df\n",
    "\n",
    "def vectorize_with_tfidf(labeled_df, train_on_entire_doc, vectorize_entire_doc, document_cut_off, max_df, min_df, ngram_range, binary= False, use_idf = True):\n",
    "    '''train_on_entire_doc = True --> determine vocabulary and stop words using entire document, else use document_cut_off\n",
    "    vectorize_on_entire_doc = True --> vectorize entire doc, otherwise only vectorize cropped document\n",
    "    document_cut_off = N --> used to crop documents to N words\n",
    "    max_df: any words appearing with frequence > max_df will be excluded\n",
    "    min_df: any words appearing with frequency < min_df will be excluded\n",
    "    ngram_range: us ngrams to create vocabulary and vectorize\n",
    "    binary = True, vecorizes documents using 1 or 0, vs word count\n",
    "    use_idf = False sets idf(t) = 1 for all tokens t.\n",
    "    '''\n",
    "    no_dups_df = drop_duplicates(labeled_df)\n",
    "    training_corpus = get_corpus(no_dups_df, train_on_entire_doc, document_cut_off)\n",
    "    vectorizing_corpus = get_corpus(labeled_df, vectorize_entire_doc, document_cut_off)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = ngram_range, min_df= min_df, max_df = max_df, binary = binary, use_idf= use_idf)\n",
    "    vectorizer.fit(training_corpus)\n",
    "    X= vectorizer.transform(vectorizing_corpus)\n",
    "    df = pd.DataFrame(X.todense(), index = labeled_df, columns = vectorizer.get_feature_names())\n",
    "    return df\n",
    "\n",
    "def allScores(X, clusters_l, labeled_df, metric = 'cosine', withJack = False):\n",
    "    sample_silhouette_values = silhouette_samples(X, clusters_l, metric= metric)\n",
    "    labeled_df_copy = labeled_df.copy()\n",
    "    #print(labeled_df_copy)\n",
    "    labeled_df_copy['sil score'] = sample_silhouette_values\n",
    "    grouped = labeled_df_copy.groupby(['labels']).mean()\n",
    "    grouped_med = labeled_df_copy.groupby(['labels']).median()\n",
    "    grouped['median_sil'] = grouped_med['sil score']\n",
    "    grouped.loc['MEAN SIL SCORE'] = grouped.mean()\n",
    "    grouped.loc['MEDIAN SIL SCORE'] = grouped.median()\n",
    "    grouped.loc['David Bouldin Score'] = davies_bouldin_score(X, clusters_l)\n",
    "    grouped.loc['Calinski Harabasz'] = calinski_harabasz_score(X, clusters_l)\n",
    "    if(withJack):\n",
    "        jack = JaccardIndex(X, clusters_l)\n",
    "        grouped.loc['Jaccard Index'] = avgJack(jack)\n",
    "    return grouped\n",
    "\n",
    "def bestScoresTFIDF(X, clusters_l, labeled_df, metric = 'cosine', withJack = False):\n",
    "    #maxDfRange = np.arange(0.6, 1.0, 0.1)\n",
    "    #minDfRange = np.arange(0, .20, 0.1)\n",
    "    #ngramRangeList = [(1,1), (1,2), (1,3), (1,4), (2,2), (2,3), (2,4), (3,3), (3,3), (3,4)]\n",
    "    maxDfRange = np.arange(0.7, 0.9, 0.05)\n",
    "    minDfRange = np.arange(0.025,0.125, 0.025)\n",
    "    ngramRangeList = [(1,1), (1,2), (1,3), (1,4)]\n",
    "    n_comp = [10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "    bestSilMean = 0\n",
    "    silMeanPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestSilMed = 0\n",
    "    silMedPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestDav = 0\n",
    "    davPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestCal = 0\n",
    "    calPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestJack  = 0\n",
    "    jackPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    for i in maxDfRange:\n",
    "        print(\"MaxDfRange:\", i)\n",
    "        maxFreq = i\n",
    "        for j in minDfRange:\n",
    "            print(\"MinDfRange:\",j)\n",
    "            minFreq = j\n",
    "            for k in range(len(ngramRangeList)):\n",
    "                print(\"Ngram Range\", k)\n",
    "                ngramRange = ngramRangeList[k]\n",
    "                \n",
    "                tfidfvectorizer = TfidfVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "                tfidf_wm = tfidfvectorizer.fit_transform(X)\n",
    "                tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "                df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = clusters_l,columns = tfidf_tokens)\n",
    "                scores = allScores(df_tfidfvect, clusters_l, textInput)\n",
    "                if(bestSilMean < scores.iloc[41][\"sil score\"]):\n",
    "                    silMeanPar[\"maxDfRange\"] = maxFreq\n",
    "                    silMeanPar[\"minDfRange\"] = minFreq\n",
    "                    silMeanPar[\"ngramRange\"] = ngramRange\n",
    "                    bestSilMean = scores.iloc[41][\"sil score\"]\n",
    "                    silMeanPar[\"reduction\"][0] = \"none\"\n",
    "                if(bestSilMed < scores.iloc[42][\"sil score\"]):\n",
    "                    silMedPar[\"maxDfRange\"] = maxFreq\n",
    "                    silMedPar[\"minDfRange\"] = minFreq\n",
    "                    silMedPar[\"ngramRange\"] = ngramRange\n",
    "                    bestSilMed = scores.iloc[42][\"sil score\"]\n",
    "                    silMedPar[\"reduction\"][0] = \"none\"\n",
    "                if(bestDav < scores.iloc[43][\"sil score\"]):\n",
    "                    davPar[\"maxDfRange\"] = maxFreq\n",
    "                    davPar[\"minDfRange\"] = minFreq\n",
    "                    davPar[\"ngramRange\"] = ngramRange\n",
    "                    bestDav = scores.iloc[43][\"sil score\"]\n",
    "                    davPar[\"reduction\"][0] = \"none\"\n",
    "                if(bestCal < scores.iloc[44][\"sil score\"]):\n",
    "                    calPar[\"maxDfRange\"] = maxFreq\n",
    "                    calPar[\"minDfRange\"] = minFreq\n",
    "                    calPar[\"ngramRange\"] = ngramRange\n",
    "                    bestCal = scores.iloc[44][\"sil score\"]\n",
    "                    calPar[\"reduction\"][0] = \"none\"\n",
    "                if(withJack and bestJack < scores.iloc[45][\"sil score\"]):\n",
    "                    jackPar[\"maxDfRange\"] = maxFreq\n",
    "                    jackPar[\"minDfRange\"] = minFreq\n",
    "                    jackPar[\"ngramRange\"] = ngramRange\n",
    "                    bestJack = scores.iloc[45][\"sil score\"]\n",
    "                    jackPar[\"reduction\"][0] = \"none\"\n",
    "                for y in range(len(n_comp)):\n",
    "                    svd = TruncatedSVD(n_components = n_comp[y])\n",
    "                    svdVect = svd.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(svdVect, clusters_l, textInput)\n",
    "                    if(bestSilMean < scores.iloc[41][\"sil score\"]):\n",
    "                        silMeanPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMeanPar[\"minDfRange\"] = minFreq\n",
    "                        silMeanPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMean = scores.iloc[41][\"sil score\"]\n",
    "                        silMeanPar[\"reduction\"][0] = \"SVD\"\n",
    "                        silMeanPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestSilMed < scores.iloc[42][\"sil score\"]):\n",
    "                        silMedPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMedPar[\"minDfRange\"] = minFreq\n",
    "                        silMedPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMed = scores.iloc[42][\"sil score\"]\n",
    "                        silMedPar[\"reduction\"][0] = \"SVD\"\n",
    "                        silMedPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestDav < scores.iloc[43][\"sil score\"]):\n",
    "                        davPar[\"maxDfRange\"] = maxFreq\n",
    "                        davPar[\"minDfRange\"] = minFreq\n",
    "                        davPar[\"ngramRange\"] = ngramRange\n",
    "                        bestDav = scores.iloc[43][\"sil score\"]\n",
    "                        davPar[\"reduction\"][0] = \"SVD\"\n",
    "                        davPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestCal < scores.iloc[44][\"sil score\"]):\n",
    "                        calPar[\"maxDfRange\"] = maxFreq\n",
    "                        calPar[\"minDfRange\"] = minFreq\n",
    "                        calPar[\"ngramRange\"] = ngramRange\n",
    "                        bestCal = scores.iloc[44][\"sil score\"]\n",
    "                        calPar[\"reduction\"][0] = \"SVD\"\n",
    "                        calPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(withJack and bestJack < scores.iloc[45][\"sil score\"]):\n",
    "                        jackPar[\"maxDfRange\"] = maxFreq\n",
    "                        jackPar[\"minDfRange\"] = minFreq\n",
    "                        jackPar[\"ngramRange\"] = ngramRange\n",
    "                        bestJack = scores.iloc[45][\"sil score\"]\n",
    "                        jackPar[\"reduction\"][0] = \"SVD\"\n",
    "                        jackPar[\"reduction\"][1] = n_comp[y]\n",
    "                    pca = PCA(n_components = n_comp[y])\n",
    "                    pcaVect = pca.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaVect, clusters_l, textInput)\n",
    "                    if(bestSilMean < scores.iloc[41][\"sil score\"]):\n",
    "                        silMeanPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMeanPar[\"minDfRange\"] = minFreq\n",
    "                        silMeanPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMean = scores.iloc[41][\"sil score\"]\n",
    "                        silMeanPar[\"reduction\"][0] = \"PCA\"\n",
    "                        silMeanPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestSilMed < scores.iloc[42][\"sil score\"]):\n",
    "                        silMedPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMedPar[\"minDfRange\"] = minFreq\n",
    "                        silMedPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMed = scores.iloc[42][\"sil score\"]\n",
    "                        silMedPar[\"reduction\"][0] = \"PCA\"\n",
    "                        silMedPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestDav < scores.iloc[43][\"sil score\"]):\n",
    "                        davPar[\"maxDfRange\"] = maxFreq\n",
    "                        davPar[\"minDfRange\"] = minFreq\n",
    "                        davPar[\"ngramRange\"] = ngramRange\n",
    "                        bestDav = scores.iloc[43][\"sil score\"]\n",
    "                        davPar[\"reduction\"][0] = \"PCA\"\n",
    "                        davPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestCal < scores.iloc[44][\"sil score\"]):\n",
    "                        calPar[\"maxDfRange\"] = maxFreq\n",
    "                        calPar[\"minDfRange\"] = minFreq\n",
    "                        calPar[\"ngramRange\"] = ngramRange\n",
    "                        bestCal = scores.iloc[44][\"sil score\"]\n",
    "                        calPar[\"reduction\"][0] = \"PCA\"\n",
    "                        calPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(withJack and bestJack < scores.iloc[45][\"sil score\"]):\n",
    "                        jackPar[\"maxDfRange\"] = maxFreq\n",
    "                        jackPar[\"minDfRange\"] = minFreq\n",
    "                        jackPar[\"ngramRange\"] = ngramRange\n",
    "                        bestJack = scores.iloc[45][\"sil score\"]\n",
    "                        jackPar[\"reduction\"][0] = \"PCA\"\n",
    "                        jackPar[\"reduction\"][1] = n_comp[y]\n",
    "                    pcaIncLower = IncrementalPCA(n_components = n_comp[y])\n",
    "                    pcaIncVect = pcaIncLower.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaIncVect, clusters_l, textInput)\n",
    "                    if(bestSilMean < scores.iloc[41][\"sil score\"]):\n",
    "                        silMeanPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMeanPar[\"minDfRange\"] = minFreq\n",
    "                        silMeanPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMean = scores.iloc[41][\"sil score\"]\n",
    "                        silMeanPar[\"reduction\"][0] = \"PCAinc\"\n",
    "                        silMeanPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestSilMed < scores.iloc[42][\"sil score\"]):\n",
    "                        silMedPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMedPar[\"minDfRange\"] = minFreq\n",
    "                        silMedPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMed = scores.iloc[42][\"sil score\"]\n",
    "                        silMedPar[\"reduction\"][0] = \"PCAinc\"\n",
    "                        silMedPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestDav < scores.iloc[43][\"sil score\"]):\n",
    "                        davPar[\"maxDfRange\"] = maxFreq\n",
    "                        davPar[\"minDfRange\"] = minFreq\n",
    "                        davPar[\"ngramRange\"] = ngramRange\n",
    "                        bestDav = scores.iloc[43][\"sil score\"]\n",
    "                        davPar[\"reduction\"][0] = \"PCAinc\"\n",
    "                        davPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestCal < scores.iloc[44][\"sil score\"]):\n",
    "                        calPar[\"maxDfRange\"] = maxFreq\n",
    "                        calPar[\"minDfRange\"] = minFreq\n",
    "                        calPar[\"ngramRange\"] = ngramRange\n",
    "                        bestCal = scores.iloc[44][\"sil score\"]\n",
    "                        calPar[\"reduction\"][0] = \"PCAinc\"\n",
    "                        calPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(withJack and bestJack < scores.iloc[45][\"sil score\"]):\n",
    "                        jackPar[\"maxDfRange\"] = maxFreq\n",
    "                        jackPar[\"minDfRange\"] = minFreq\n",
    "                        jackPar[\"ngramRange\"] = ngramRange\n",
    "                        bestJack = scores.iloc[45][\"sil score\"]\n",
    "                        jackPar[\"reduction\"][0] = \"PCAinc\"\n",
    "                        jackPar[\"reduction\"][1] = n_comp[y]\n",
    "                    pcaKernelReduce = KernelPCA(n_components = n_comp[y])\n",
    "                    pcaKernel = pcaKernelReduce.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaKernel, clusters_l, textInput)\n",
    "                    if(bestSilMean < scores.iloc[41][\"sil score\"]):\n",
    "                        silMeanPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMeanPar[\"minDfRange\"] = minFreq\n",
    "                        silMeanPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMean = scores.iloc[41][\"sil score\"]\n",
    "                        silMeanPar[\"reduction\"][0] = \"PCAKernel\"\n",
    "                        silMeanPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestSilMed < scores.iloc[42][\"sil score\"]):\n",
    "                        silMedPar[\"maxDfRange\"] = maxFreq\n",
    "                        silMedPar[\"minDfRange\"] = minFreq\n",
    "                        silMedPar[\"ngramRange\"] = ngramRange\n",
    "                        bestSilMed = scores.iloc[42][\"sil score\"]\n",
    "                        silMedPar[\"reduction\"][0] = \"PCAKernel\"\n",
    "                        silMedPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestDav > scores.iloc[43][\"sil score\"]):\n",
    "                        davPar[\"maxDfRange\"] = maxFreq\n",
    "                        davPar[\"minDfRange\"] = minFreq\n",
    "                        davPar[\"ngramRange\"] = ngramRange\n",
    "                        bestDav = scores.iloc[43][\"sil score\"]\n",
    "                        davPar[\"reduction\"][0] = \"PCAKernel\"\n",
    "                        davPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(bestCal < scores.iloc[44][\"sil score\"]):\n",
    "                        calPar[\"maxDfRange\"] = maxFreq\n",
    "                        calPar[\"minDfRange\"] = minFreq\n",
    "                        calPar[\"ngramRange\"] = ngramRange\n",
    "                        bestCal = scores.iloc[44][\"sil score\"]\n",
    "                        calPar[\"reduction\"][0] = \"PCAKernel\"\n",
    "                        calPar[\"reduction\"][1] = n_comp[y]\n",
    "                    if(withJack and bestJack < scores.iloc[45][\"sil score\"]):\n",
    "                        jackPar[\"maxDfRange\"] = maxFreq\n",
    "                        jackPar[\"minDfRange\"] = minFreq\n",
    "                        jackPar[\"ngramRange\"] = ngramRange\n",
    "                        bestJack = scores.iloc[45][\"sil score\"]\n",
    "                        jackPar[\"reduction\"][0] = \"PCAKernel\"\n",
    "                        jackPar[\"reduction\"][1] = n_comp[y]\n",
    "    if(withJack):\n",
    "        r1 = [silMeanPar, silMedPar, davPar, calPar, jackPar]\n",
    "        r2 = [bestSilMean, bestSilMed, bestDav, bestCal, bestJack]\n",
    "        results = [r1,r2]\n",
    "    else:\n",
    "        r1 = [silMeanPar, silMedPar, davPar, calPar]\n",
    "        r2 = [bestSilMean, bestSilMed, bestDav, bestCal]\n",
    "        results = [r1,r2]\n",
    "    return results\n",
    "\n",
    "def avgJack(jack):\n",
    "    totalJackAvg = 0\n",
    "    totalJackVar = 0\n",
    "    jackSum = summary(jack.score_df)\n",
    "    for i in range(len(jackSum)):\n",
    "        totalJackAvg = totalJackAvg + jackSum.iloc[i]['Avg Score']\n",
    "        totalJackVar = totalJackVar + jackSum.iloc[i]['Variance']\n",
    "    return (totalJackAvg/ len(jackSum), totalJackVar/len(jackSum))\n",
    "\n",
    "def remove_page_nos(text):\n",
    "    new_text = re.subn(r'Page \\d*', '', text)\n",
    "    return new_text[0]\n",
    "    \n",
    "def remove_metadata(text):\n",
    "    parts = text.split('Page 1\\n')\n",
    "    if len(parts)>1:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def truncate(number, digits) -> float:\n",
    "    # Improve accuracy with floating point operations, to avoid truncate(16.4, 2) = 16.39 or truncate(-1.13, 2) = -1.12\n",
    "    nbDecimals = len(str(number).split('.')[1]) \n",
    "    if nbDecimals <= digits:\n",
    "        return number\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper\n",
    "\n",
    "def printSamples(df, labels):\n",
    "    samples = silhouette_samples(df, labels, metric = \"cosine\")\n",
    "    types = list(set(labels))\n",
    "    dict = {}\n",
    "    totalSum = []\n",
    "    meanSum = []\n",
    "    medianSum = []\n",
    "    for label in types:\n",
    "        dict[label] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(samples[i])\n",
    "    for key, value in dict.items():\n",
    "        totalSum = totalSum + list(value)\n",
    "        meanSum.append(truncate(statistics.mean(value), 4))\n",
    "        medianSum.append(truncate(statistics.median(value),4))\n",
    "        print(\"{:<30} {:<15} {:<15}\".format(key,truncate(statistics.mean(value), 4), truncate(statistics.median(value),4)))\n",
    "    #print(key, \": \", statistics.mean(value))\n",
    "    print(\"{:<40} {:<15} {:<15}\".format(\"Score Types:\", \"Silhouettes Mean\", \"Sihouettes Medina\"))\n",
    "    print(\"{:<40} {:<15} {:<15}\".format(\"Mean & Median\", truncate(statistics.mean(totalSum), 4), truncate(statistics.median(totalSum), 4)))\n",
    "    print(\"{:<40} {:<15} {:<15}\".format(\"Mean of mean & Median of Median\", truncate(statistics.mean(meanSum),4), truncate(statistics.median(medianSum), 4)))\n",
    "\n",
    "def printMeanMedian(df, labels):\n",
    "    samples = silhouette_samples(df, labels, metric = \"cosine\")\n",
    "    types = list(set(labels))\n",
    "    dict = {}\n",
    "    totalSum = []\n",
    "    meanSum = []\n",
    "    medianSum = []\n",
    "    for label in types:\n",
    "        dict[label] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(samples[i])\n",
    "    for key, value in dict.items():\n",
    "        totalSum = totalSum + list(value)\n",
    "        meanSum.append(statistics.mean(value))\n",
    "        medianSum.append(statistics.median(value))\n",
    "    print(\"{:<30} {:<15} {:<15}\".format(\"Mean & Median\", statistics.mean(totalSum), statistics.median(totalSum)))\n",
    "    print(\"{:<30} {:<15} {:<15}\".format(\"Mean of mean & Median of Median\", statistics.mean(meanSum), statistics.median(medianSum)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstemmer = PorterStemmer()\\nplurals = [\"dies\", \"buses\", \"bus\", \"ran\", \"running\"]\\nstemText = text_l.copy()\\nprint(text_l[0].sp)\\nfor i in range(len(text_l)):\\n    words = text_l[i].split();\\n    for j in range(len(words)):\\n        stemText[i] = stemmer.stem(text_l[i][j])\\nstemText[i]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Data\n",
    "file_path_l = []\n",
    "\n",
    "for root, dirs, files in os.walk('TextClustersForVectorizationAlexis'):\n",
    "    for filename in files:\n",
    "        file_path_l.append(os.path.join(root, filename))\n",
    "\n",
    "text_l = []\n",
    "for file_path in file_path_l:\n",
    "    filey = open(file_path,encoding=\"mbcs\")\n",
    "    text_l.append(filey.read())\n",
    "    filey.close()\n",
    "\n",
    "types_l = []\n",
    "for namey in file_path_l:\n",
    "    splitted = namey.split('\\\\')\n",
    "    types_l.append(splitted[1])\n",
    "'''\n",
    "stemmer = PorterStemmer()\n",
    "plurals = [\"dies\", \"buses\", \"bus\", \"ran\", \"running\"]\n",
    "stemText = text_l.copy()\n",
    "print(text_l[0].sp)\n",
    "for i in range(len(text_l)):\n",
    "    words = text_l[i].split();\n",
    "    for j in range(len(words)):\n",
    "        stemText[i] = stemmer.stem(text_l[i][j])\n",
    "stemText[i]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Cropped Data to Normal Data\n",
    "training_df = pd.read_csv('InternSampleDataSmall.csv')\n",
    "training_df = training_df.drop(columns = ['Unnamed: 0'])\n",
    "labels = training_df['labels']\n",
    "numWords = 30\n",
    "\n",
    "tfidf_vectorized_df = vectorize_with_tfidf(training_df, False, False, numWords, .8, .005, (1,2), False, True)\n",
    "#print(silhouette_score(tfidf_vectorized_df, labels, metric = 'cosine'))\n",
    "\"\"\"print(tfidf_vectorized_df)\n",
    "print()\n",
    "print(labels)\"\"\"\n",
    "#print(allScores(tfidf_vectorized_df, labels, training_df, metric = 'cosine'))\n",
    "print(training_df)\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "SVDtfidfvect = svd.fit_transform(tfidf_vectorized_df)\n",
    "#printSamples(SVDtfidfvect, labels)\n",
    "\n",
    "no_dups_df = drop_duplicates(training_df)\n",
    "training_corpus = get_corpus(no_dups_df, False, numWords)\n",
    "vectorizing_corpus = get_corpus(training_df, False, numWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'textInputCut = get_corpus(textInput, False, numWords)\\nprint(textInputCut)\\ncleanTextDF = pd.DataFrame(data = textInputCut, index = types_l)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanText = []\n",
    "for i in range(len(text_l)):\n",
    "    cleanText.append(remove_metadata(remove_page_nos(text_l[i])))\n",
    "\n",
    "d = {\"labels\": types_l, \"text\": cleanText}\n",
    "textInput = pd.DataFrame(data = d)\n",
    "\"\"\"textInputCut = get_corpus(textInput, False, numWords)\n",
    "print(textInputCut)\n",
    "cleanTextDF = pd.DataFrame(data = textInputCut, index = types_l)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               00       000  0001  000â  001  002  003  004  \\\n",
      "Alcohol Control -- good  0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "Alcohol Control -- good  0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "Alcohol Control -- good  0.000000  0.040769   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "Alcohol Control -- good  0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "Alcohol Control -- good  0.022229  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "...                           ...       ...   ...   ...  ...  ...  ...  ...   \n",
      "StreetPermits-- good     0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "StreetPermits-- good     0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "StreetPermits-- good     0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "StreetPermits-- good     0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "StreetPermits-- good     0.000000  0.000000   0.0   0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                         005  006  ...  years  yearâ  yes  york  youth  zone  \\\n",
      "Alcohol Control -- good  0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "Alcohol Control -- good  0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "Alcohol Control -- good  0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "Alcohol Control -- good  0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "Alcohol Control -- good  0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "...                      ...  ...  ...    ...    ...  ...   ...    ...   ...   \n",
      "StreetPermits-- good     0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "StreetPermits-- good     0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "StreetPermits-- good     0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "StreetPermits-- good     0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "StreetPermits-- good     0.0  0.0  ...    0.0    0.0  0.0   0.0    0.0   0.0   \n",
      "\n",
      "                         zoning  zoom   â½  œcontractâ  \n",
      "Alcohol Control -- good     0.0   0.0  0.0         0.0  \n",
      "Alcohol Control -- good     0.0   0.0  0.0         0.0  \n",
      "Alcohol Control -- good     0.0   0.0  0.0         0.0  \n",
      "Alcohol Control -- good     0.0   0.0  0.0         0.0  \n",
      "Alcohol Control -- good     0.0   0.0  0.0         0.0  \n",
      "...                         ...   ...  ...         ...  \n",
      "StreetPermits-- good        0.0   0.0  0.0         0.0  \n",
      "StreetPermits-- good        0.0   0.0  0.0         0.0  \n",
      "StreetPermits-- good        0.0   0.0  0.0         0.0  \n",
      "StreetPermits-- good        0.0   0.0  0.0         0.0  \n",
      "StreetPermits-- good        0.0   0.0  0.0         0.0  \n",
      "\n",
      "[323 rows x 3710 columns]\n",
      "0.3616672173469206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"totalJackAvg = 0\\ntotalJackVar = 0\\njackSum = summary(jack.score_df)\\n\\nfor i in range(len(jackSum)):\\n    totalJackAvg = totalJackAvg + jackSum.iloc[i]['Avg Score']\\n    totalJackVar = totalJackVar + jackSum.iloc[i]['Variance']\\nprint(totalJackAvg/len(jackSum))\\nprint(totalJackVar/len(jackSum))\\nprint(jackSum)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#My countVectorize and TFIDFVectorize\n",
    "maxFreq = .9\n",
    "minFreq = .05\n",
    "ngramRange = (1,1)\n",
    "#countvectorizer = CountVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "tfidfvectorizer = TfidfVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "#tfidfAlexis = TfidfVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "#tfidfCut = TfidfVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "\n",
    "#count_wm = countvectorizer.fit_transform(cleanText)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(cleanText)\n",
    "#print(count_wm.toarray())\n",
    "#tfidfAlexis.fit(training_corpus)\n",
    "#X = tfidfAlexis.transform(vectorizing_corpus)\n",
    "#tfidfCut_wm = tfidfCut.fit_transform(cleanText)\n",
    "\n",
    "#count_tokens = countvectorizer.get_feature_names_out()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "#tfidfAlexis_tokens = tfidfAlexis.get_feature_names_out()\n",
    "#tfidfCut_tokens = tfidfCut.get_feature_names_out()\n",
    "\n",
    "#df_countvect = pd.DataFrame(data = count_wm.toarray(),index = types_l,columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = types_l,columns = tfidf_tokens)\n",
    "print(df_tfidfvect)\n",
    "score = silhouette_score(df_tfidfvect, types_l, metric = \"cosine\")\n",
    "print(score)\n",
    "#df_tfidfAlexis = pd.DataFrame(X.todense(), index = labels, columns = tfidfAlexis.get_feature_names())\n",
    "#df_tfidfCut = pd.DataFrame(data = tfidfCut_wm.toarray(), index = types_l, columns = tfidfCut_tokens)\n",
    "#print(df_tfidfvect)\n",
    "#jack = JaccardIndex(df_tfidfvect, types_l)\n",
    "#print(jack.display())\n",
    "\"\"\"totalJackAvg = 0\n",
    "totalJackVar = 0\n",
    "jackSum = summary(jack.score_df)\n",
    "\n",
    "for i in range(len(jackSum)):\n",
    "    totalJackAvg = totalJackAvg + jackSum.iloc[i]['Avg Score']\n",
    "    totalJackVar = totalJackVar + jackSum.iloc[i]['Variance']\n",
    "print(totalJackAvg/len(jackSum))\n",
    "print(totalJackVar/len(jackSum))\n",
    "print(jackSum)\"\"\"\n",
    "#d = {\"labels\": types_l, \"text\": cleanText}\n",
    "#textInput = pd.DataFrame(data = d)\n",
    "#scores = allScores(df_tfidfvect, types_l, textInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordResultsSmall = []\n",
    "\n",
    "#wordCountSmall = [30]\n",
    "wordCountSmall = [30,70,100]\n",
    "for word in wordCountSmall:\n",
    "    print(\"Words:\", word)\n",
    "    textInputCut = get_corpus(textInput, False, word)\n",
    "    d2 = {\"labels\": types_l, \"text\": textInputCut}\n",
    "    #cleanTextDF = pd.DataFrame(data = textInputCut, index = types_l)\n",
    "    cleanTextDFsil = pd.DataFrame(data = d2)\n",
    "    wordResultsSmall.append(bestScoresTFIDF(textInputCut, cleanTextDFsil['labels'], cleanTextDFsil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scores.iloc[44][\"sil score\"])\n",
    "#print(scores)\n",
    "\n",
    "results = bestScoresTFIDF(cleanText, types_l, textInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in results[0]:\n",
    "    print(\"\")\n",
    "    for key, value in x.items():\n",
    "        print(key, \": \", value)\n",
    "\n",
    "for y in results[1]:\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"one\": 1, \"two\": 2}\n",
    "def changeDic(dic):\n",
    "    dic[\"one\"] = 2\n",
    "changeDic(dic)\n",
    "for key, value in dic.items():\n",
    "        print(key, \": \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette Scores\n",
    "score = silhouette_score(df_tfidfvect, types_l, metric = \"cosine\")\n",
    "#print(\"TFIDF :\", score)\n",
    "\n",
    "samples = silhouette_samples(df_tfidfvect, types_l, metric = \"cosine\")\n",
    "printSamples(df_tfidfvect, types_l)\n",
    "\n",
    "#print(silhouette_scores(df_tfidfAlexis, labels, training_df, 'cosine'))\n",
    "score = silhouette_score(df_tfidfAlexis, labels, metric = \"cosine\")\n",
    "#print(\"TFIDF Alexis:\", score)\n",
    "print(silhouette_scores(df_tfidfAlexis, labels,training_df, 'cosine'))\n",
    "score = silhouette_score(df_tfidfCut, types_l, metric = \"cosine\")\n",
    "#print(\"TFIDF Cut:\", score)\n",
    "\n",
    "score = silhouette_score(bert_df, types_l, metric = \"cosine\")\n",
    "#print(\"BERT :\", score)\n",
    "\n",
    "\n",
    "score = silhouette_score(df_combined, types_l, metric = \"cosine\")\n",
    "#print(\"Combined :\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Evaluate Dimension Reduction\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "SVDtfidfvect = svd.fit_transform(df_tfidfvect)\n",
    "SVDtfidfAlexis = svd.fit_transform(df_tfidfAlexis)\n",
    "SVDtfidfCut = svd.fit_transform(df_tfidfCut)\n",
    "\n",
    "\"\"\"print(\"TFIDF SVD:\", silhouette_score(SVDtfidfvect, types_l, metric = \"cosine\"))\n",
    "print(\"TFIDF\")\n",
    "printMeanMedian(SVDtfidfvect, types_l)\n",
    "print(\"TFIDF Alexis\")\n",
    "printMeanMedian(SVDtfidfAlexis, labels)\n",
    "print(\"TFIDF Cut OFf\")\n",
    "printMeanMedian(SVDtfidfCut,types_l)\n",
    "print()\"\"\"\n",
    "#print(\"TFIDF Alexis SVD:\", silhouette_score(SVDtfidfAlexis, labels))\n",
    "#print(\"TFIDF Cut SVD:\", silhouette_score(SVDtfidfCut, types_l))\n",
    "\n",
    "pca = PCA(n_components= 20)\n",
    "PCAtfidfVect = pca.fit_transform(df_tfidfvect)\n",
    "PCAtfidfAlexis = pca.fit_transform(df_tfidfAlexis)\n",
    "PCAtfidfCut = pca.fit_transform(df_tfidfCut)\n",
    "print(\"TFIDF PCA:\", silhouette_score(SVDtfidfvect, types_l, metric = \"cosine\"))\n",
    "print(\"TFIDF\")\n",
    "printMeanMedian(PCAtfidfVect, types_l)\n",
    "print(\"TFIDF Alexis\")\n",
    "printMeanMedian(PCAtfidfAlexis, labels)\n",
    "print(\"TFIDF Cut OFf\")\n",
    "printMeanMedian(PCAtfidfCut,types_l)\n",
    "print(PCAtfidfVect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode BERT\n",
    "model = SentenceTransformer('msmarco-distilbert-cos-v5')\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "dataset = api.load('text8')\n",
    "datasetList= list(dataset)\n",
    "data = [d for d in dataset]\n",
    "#print(datasetList[0])\n",
    "doc_emb = model.encode(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF shapes\n",
    "print(\"TFIDF Dataframe shape:\", df_tfidfvect.shape)\n",
    "bert_df = pd.DataFrame(doc_emb, index = types_l)\n",
    "#print(bert_df)\n",
    "print(\"BERT Datframe shape:\", bert_df.shape)\n",
    "#print(df_tfidfvect)\n",
    "df_combined = pd.concat([df_tfidfvect, bert_df], axis = 1, join = 'outer')\n",
    "print(\"Combined Shape:\",df_combined.shape)\n",
    "printSamples(df_combined, types_l)\n",
    "print()\n",
    "SVDBert = svd.fit_transform(bert_df)\n",
    "SVDCombined = pd.concat([df_tfidfvect, SVDBert], axis = 1, join = 'outer')\n",
    "#printSamples(SVDCombined, types_l)\n",
    "\n",
    "#I have been going back and forth between the different dimension reductions and combining BERT\n",
    "#BERT has not been working as well as I hoped but I am going to see if removing non-English characters may help the predicitons better. \n",
    "#I was also going to try implementing stemming today and see if that helps the TFIDF models\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4bec7b648d61350689a0ccb93b53155b5d49d036bbaf30ce84901e3a87cea97"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
