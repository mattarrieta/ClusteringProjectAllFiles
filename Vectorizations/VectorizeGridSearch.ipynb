{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import os, itertools\n",
    "import warnings\n",
    "from matplotlib.pyplot import text\n",
    "from numpy import vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statistics\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import math\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "#from JaccardIndexUpdated import JaccardIndex, ClustersData, summary\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def drop_duplicates(labeled_df):\n",
    "    data_copy_df = labeled_df.copy()\n",
    "    data_no_duplicates = data_copy_df.drop_duplicates(subset = ['text'])\n",
    "    return data_no_duplicates\n",
    "\n",
    "def crop_documents(labeled_df, document_cutoff_length):\n",
    "    '''Returns a df with 'cropped_documents' columns'''\n",
    "    labeled_df_copy = labeled_df.copy()\n",
    "    labeled_df_copy['tokens_cropped'] = labeled_df_copy['text'].apply(lambda x: x.split()[:document_cutoff_length])\n",
    "    labeled_df_copy['cropped_documents'] = labeled_df_copy['tokens_cropped'].apply(lambda x: ' '.join(x))\n",
    "    labeled_df_copy = labeled_df_copy.drop(columns = ['tokens_cropped'])\n",
    "    return labeled_df_copy\n",
    "\n",
    "def get_corpus(labeled_df, on_entire_doc, document_cut_off):\n",
    "    if on_entire_doc == False:\n",
    "        crop_documents_df = crop_documents(labeled_df, document_cut_off)\n",
    "        corpus_to_train_on = crop_documents_df['cropped_documents'].to_list()\n",
    "    else:\n",
    "        corpus_to_train_on = labeled_df['text'].to_list()\n",
    "    return corpus_to_train_on\n",
    "\n",
    "def numUniqueLabels(cluster_l):\n",
    "    from collections import Counter\n",
    "    items = Counter(cluster_l).keys()\n",
    "    return len(items)\n",
    "\n",
    "def allScores(X, clusters_l, labeled_df, metric = 'cosine', withJack = False):\n",
    "    sample_silhouette_values = silhouette_samples(X, clusters_l, metric= metric)\n",
    "    labeled_df_copy = labeled_df.copy()\n",
    "    #print(labeled_df_copy)\n",
    "    labeled_df_copy['sil score'] = sample_silhouette_values\n",
    "    grouped = labeled_df_copy.groupby(['labels']).mean()\n",
    "    grouped_med = labeled_df_copy.groupby(['labels']).median()\n",
    "    grouped['median_sil'] = grouped_med['sil score']\n",
    "    grouped.loc['MEAN SIL SCORE'] = grouped.mean()\n",
    "    grouped.loc['MEDIAN SIL SCORE'] = grouped.median()\n",
    "    grouped.loc['David Bouldin Score'] = davies_bouldin_score(X, clusters_l)\n",
    "    grouped.loc['Calinski Harabasz'] = calinski_harabasz_score(X, clusters_l)\n",
    "    if(withJack):\n",
    "        jack = JaccardIndex(X, clusters_l)\n",
    "        grouped.loc['Jaccard Index'] = avgJack(jack)\n",
    "    return grouped\n",
    "\n",
    "def checkItems(scores, bestItems, bestParameters, dimensionReduction, numComponents, numLabels, maxFreq, minFreq, ngramRange, withJack):\n",
    "    \n",
    "    if(bestItems[0] < scores.iloc[numLabels][\"sil score\"]):\n",
    "        bestParameters[0][\"maxDfRange\"] = maxFreq\n",
    "        bestParameters[0][\"minDfRange\"] = minFreq\n",
    "        bestParameters[0][\"ngramRange\"] = ngramRange\n",
    "        bestItems[0] = scores.iloc[numLabels][\"sil score\"]\n",
    "        bestParameters[0][\"reduction\"][0] = dimensionReduction\n",
    "        bestParameters[0][\"reduction\"][1] = numComponents\n",
    "    if(bestItems[1] < scores.iloc[numLabels + 1][\"sil score\"]):\n",
    "        bestParameters[1][\"maxDfRange\"] = maxFreq\n",
    "        bestParameters[1][\"minDfRange\"] = minFreq\n",
    "        bestParameters[1][\"ngramRange\"] = ngramRange\n",
    "        bestItems[1] = scores.iloc[numLabels + 1][\"sil score\"]\n",
    "        bestParameters[1][\"reduction\"][0] = dimensionReduction\n",
    "        bestParameters[1][\"reduction\"][1] = numComponents\n",
    "    if(bestItems[2] > scores.iloc[numLabels + 2][\"sil score\"]):\n",
    "        bestParameters[2][\"maxDfRange\"] = maxFreq\n",
    "        bestParameters[2][\"minDfRange\"] = minFreq\n",
    "        bestParameters[2][\"ngramRange\"] = ngramRange\n",
    "        bestItems[2] = scores.iloc[numLabels + 2][\"sil score\"]\n",
    "        bestParameters[2][\"reduction\"][0] = dimensionReduction\n",
    "        bestParameters[2][\"reduction\"][1] = numComponents\n",
    "    if(bestItems[3] < scores.iloc[numLabels + 3][\"sil score\"]):\n",
    "        bestParameters[3][\"maxDfRange\"] = maxFreq\n",
    "        bestParameters[3][\"minDfRange\"] = minFreq\n",
    "        bestParameters[3][\"ngramRange\"] = ngramRange\n",
    "        bestItems[3] = scores.iloc[numLabels + 3][\"sil score\"]\n",
    "        bestParameters[3][\"reduction\"][0] = dimensionReduction\n",
    "        bestParameters[3][\"reduction\"][1] = numComponents\n",
    "    if(withJack and bestItems[4] < scores.iloc[numLabels + 4][\"sil score\"]):\n",
    "        bestParameters[4][\"maxDfRange\"] = maxFreq\n",
    "        bestParameters[4][\"minDfRange\"] = minFreq\n",
    "        bestParameters[4][\"ngramRange\"] = ngramRange\n",
    "        bestItems[4] = scores.iloc[numLabels + 4][\"sil score\"]\n",
    "        bestParameters[4][\"reduction\"][0] = dimensionReduction\n",
    "        bestParameters[4][\"reduction\"][1] = numComponents\n",
    "\n",
    "def avgJack(jack):\n",
    "    totalJackAvg = 0\n",
    "    totalJackVar = 0\n",
    "    jackSum = summary(jack.score_df)\n",
    "    for i in range(len(jackSum)):\n",
    "        totalJackAvg = totalJackAvg + jackSum.iloc[i]['Avg Score']\n",
    "        totalJackVar = totalJackVar + jackSum.iloc[i]['Variance']\n",
    "    return (totalJackAvg/ len(jackSum), totalJackVar/len(jackSum))\n",
    "\n",
    "\n",
    "def bestScoresTFIDF(X, clusters_l, labeled_df, metric = 'cosine', withJack = False, printProgress = False):\n",
    "    #ngramRangeList = [(1,1), (1,2), (1,3), (1,4), (2,2), (2,3), (2,4), (3,3), (3,3), (3,4)]\n",
    "    ngramRangeList = [(1,2)]\n",
    "    #maxDfRange = np.arange(0.7, 0.9, 0.05)\n",
    "    maxDfRange = [0.8]\n",
    "    #minDfRange = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "    mindDfRange = [0.005]\n",
    "    n_comp = [10]\n",
    "\n",
    "    numLabels = numUniqueLabels(clusters_l)\n",
    "    bestSilMean = 0\n",
    "    silMeanPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestSilMed = 0\n",
    "    silMedPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestDav = 100\n",
    "    davPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestCal = 0\n",
    "    calPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestJack  = 0\n",
    "    jackPar = {\"maxDfRange\": 0, \"minDfRange\": 0, \"ngramRange\": (0,0), \"reduction\": [\"none\", 0]}\n",
    "    bestItems = [bestSilMean, bestSilMed, bestDav, bestCal, bestJack]\n",
    "    bestParameters = [silMeanPar, silMedPar, davPar, calPar, jackPar]\n",
    "    for i in maxDfRange:\n",
    "        if(printProgress):\n",
    "            print(\"MaxDfRange:\", i)\n",
    "        maxFreq = i\n",
    "        for j in minDfRange:\n",
    "            if(printProgress):\n",
    "                print(\"MinDfRange:\",j)\n",
    "            minFreq = j\n",
    "            for k in range(len(ngramRangeList)):\n",
    "                if(printProgress):\n",
    "                    print(\"Ngram Range\", k)\n",
    "                ngramRange = ngramRangeList[k]\n",
    "                \n",
    "                tfidfvectorizer = TfidfVectorizer(stop_words= 'english', lowercase = True, max_df = maxFreq, min_df = minFreq, ngram_range=ngramRange)\n",
    "                tfidf_wm = tfidfvectorizer.fit_transform(X)\n",
    "                tfidf_tokens = tfidfvectorizer.get_feature_names_out()\n",
    "                df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = clusters_l,columns = tfidf_tokens)\n",
    "                scores = allScores(df_tfidfvect, clusters_l, labeled_df, metric = metric)\n",
    "                checkItems(scores, bestItems, bestParameters, \"none\", 0, numLabels, maxFreq, minFreq, ngramRange, withJack)\n",
    "                #print(bestItems)\n",
    "                for y in range(len(n_comp)):\n",
    "                    if(printProgress):\n",
    "                        print(\"SVD\")\n",
    "                    svd = TruncatedSVD(n_components = n_comp[y])\n",
    "                    svdVect = svd.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(svdVect, clusters_l, labeled_df)\n",
    "                    checkItems(scores, bestItems, bestParameters, \"SVD\", n_comp[y], numLabels, maxFreq, minFreq, ngramRange, withJack)\n",
    "                    \n",
    "                    if(printProgress):\n",
    "                        print(\"PCA\")\n",
    "                    pca = PCA(n_components = n_comp[y])\n",
    "                    pcaVect = pca.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaVect, clusters_l, labeled_df)\n",
    "                    checkItems(scores, bestItems, bestParameters, \"PCA\", n_comp[y], numLabels, maxFreq, minFreq, ngramRange, withJack)\n",
    "                    \n",
    "                    if(printProgress):\n",
    "                        print(\"PCALower\")\n",
    "                    pcaIncLower = IncrementalPCA(n_components = n_comp[y])\n",
    "                    pcaIncVect = pcaIncLower.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaIncVect, clusters_l, labeled_df)\n",
    "                    checkItems(scores, bestItems, bestParameters, \"PCAinc\", n_comp[y], numLabels, maxFreq, minFreq, ngramRange, withJack)\n",
    "                    if(printProgress):\n",
    "                        print(\"PCA Kernel\")\n",
    "                    pcaKernelReduce = KernelPCA(n_components = n_comp[y])\n",
    "                    pcaKernel = pcaKernelReduce.fit_transform(df_tfidfvect)\n",
    "                    scores = allScores(pcaKernel, clusters_l, labeled_df)\n",
    "                    checkItems(scores, bestItems, bestParameters, \"PCAKernel\", n_comp[y], numLabels, maxFreq, minFreq, ngramRange, withJack)\n",
    "    \n",
    "    if(withJack):\n",
    "        results = [bestParameters, bestItems]\n",
    "    else:\n",
    "        results = [bestParameters[:-1], bestItems[:-1]]\n",
    "\n",
    "    return results\n",
    "\n",
    "def gridSearchReduction(text_l, labels, testFileSize = [], testFullLength = False, printProgress = False):\n",
    "    results = []\n",
    "    d = {\"labels\": labels, \"text\": text_l}\n",
    "    textInput = pd.DataFrame(data = d)\n",
    "    for word in testFileSize:\n",
    "        textInputCut = get_corpus(textInput, False, word)\n",
    "        d2 = {\"labels\": labels, \"text\": textInputCut}\n",
    "        cleanTextDFsil = pd.DataFrame(data = d2)\n",
    "        results.append(bestScoresTFIDF(textInputCut, cleanTextDFsil['labels'], cleanTextDFsil, printProgress = printProgress))\n",
    "    d = {\"labels\": labels, \"text\": text_l}\n",
    "    textInput = pd.DataFrame(data = d)\n",
    "    results.append(bestScoresTFIDF(text_l,textInput[\"labels\"], textInput))\n",
    "    return results\n",
    "\n",
    "def printResults(optimalParameters, docLengthTest):\n",
    "    parameterTypes = [\"Silhouette Mean\", \"Silhouette Median\", \"Davies-Bouldin\", \"Calinski-Harabasz\"]\n",
    "    print()\n",
    "    for i in range(len(docLengthTest)):\n",
    "        print(\"Words: \", docLengthTest[i])\n",
    "        for x in range(len(optimalParameters[i][0])):\n",
    "            print(parameterTypes[x])\n",
    "            for key, value in optimalParameters[i][0][x].items():\n",
    "                print(key, \": \", value)\n",
    "\n",
    "            print(\"Score:\", optimalParameters[i][1][x])\n",
    "            print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TextInput')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TestFiles')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\Vectorizations')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\Clustering')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\KeywordExtraction')\n",
    "from CleanText import getCleanText, getText\n",
    "\n",
    "text_l, labels = getCleanText(r\"C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TestFiles\\KeywordFilesTight\", RemoveNums = True, lem=True, extendStopWords= False)\n",
    "d = {\"labels\": labels, \"text\": text_l}\n",
    "textInput = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words:  50\n",
      "Silhouette Mean\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.005\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 0.6456227720715029\n",
      "\n",
      "Silhouette Median\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.02\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCA', 10]\n",
      "Score: 0.7672778976873513\n",
      "\n",
      "Davies-Bouldin\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.01\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 0.7410108376236718\n",
      "\n",
      "Calinski-Harabasz\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.02\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 95.8222731419529\n",
      "\n",
      "Words:  75\n",
      "Silhouette Mean\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.005\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 0.5748289364488502\n",
      "\n",
      "Silhouette Median\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.05\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 0.7174195159573156\n",
      "\n",
      "Davies-Bouldin\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.02\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAinc', 10]\n",
      "Score: 0.915907154269799\n",
      "\n",
      "Calinski-Harabasz\n",
      "maxDfRange :  0.8\n",
      "minDfRange :  0.01\n",
      "ngramRange :  (1, 2)\n",
      "reduction :  ['PCAKernel', 10]\n",
      "Score: 86.09626675339699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramRangeList = [(1,2),(1,4)]\n",
    "maxDfRange = np.arange(0.7, 0.9, 0.05)\n",
    "minDfRange = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "numComponentRange = [10, 20]\n",
    "docLengthTests = [50, 75]\n",
    "\n",
    "optimalParameters = gridSearchReduction(text_l, labels, docLengthTests, False, printProgress= False)\n",
    "\n",
    "printResults(optimalParameters, docLengthTests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc6a87218d93c3eb3aa29305b960fd7fc714f48d8ac50960275ef4f8856ba0fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
