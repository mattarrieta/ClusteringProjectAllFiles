{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import sys\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TextInput')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TestFiles')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\Vectorizations')\n",
    "sys.path.insert(0, r'C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\Clustering')\n",
    "from ClusterHDBSCAN import clusterHDBSCANNoLabels, clusterHDBSCANLabels, groupLabels\n",
    "\n",
    "\n",
    "import os, itertools\n",
    "import warnings\n",
    "from matplotlib.pyplot import text\n",
    "from numpy import vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statistics\n",
    "import math\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "#from JaccardIndexUpdated import JaccardIndex, ClustersData, summary\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "#from VectorizeHelperFunction import allScores\n",
    "from CleanText import getCleanText, getText\n",
    "from vectorize import vectorizeTFIDF, vectorizeTFIDFUMAP, vectorizeTFIDFSVD\n",
    "import umap\n",
    "import umap.plot\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import datashader.bundling as bd\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "import bokeh.plotting as bpl\n",
    "import bokeh.transform as btr\n",
    "from django.urls import re_path as url\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd \n",
    "import umap\n",
    "import umap.plot\n",
    "import hdbscan\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from gensim.models import LdaModel, HdpModel\n",
    "from gensim import corpora\n",
    "from LDAKeywordExtraction import keywordsResultsTestingLDA, keywordResultsClusterLDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_l = []\n",
    "for root, dirs, files in os.walk(r\"C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TestFiles\\KeywordFilesTight\"):\n",
    "    for filename in files:\n",
    "        print(root)\n",
    "        file_path_l.append(os.path.join(root, filename))\n",
    "\n",
    "text_l = []\n",
    "for file_path in file_path_l:\n",
    "    filey = open(file_path,encoding=\"mbcs\")\n",
    "    text_l.append(filey.read())\n",
    "    filey.close()\n",
    "\n",
    "outlier_num = 0\n",
    "types_l = []\n",
    "for namey in file_path_l:\n",
    "    splitted = namey.split('\\\\')\n",
    "    print(splitted[7])\n",
    "    if splitted[7] == 'Outliers':\n",
    "        # text_labels.append(file_path.split('\\\\')[-2])\n",
    "        types_l.append(f\"Outlier-{outlier_num}\")\n",
    "        outlier_num += 1\n",
    "    else:\n",
    "        types_l.append(splitted[7])\n",
    "print(types_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_l, labels = getCleanText(r\"C:\\Users\\Matthew Arrieta\\Desktop\\Project3Testing\\TestFiles\\KeywordFilesTight\", RemoveNums = True, lem=True, extendStopWords= False)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Raw Text\n",
    "rawText, rawlabels = getText(\"KeywordFilesTight\")\n",
    "#print(rawText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizeTFIDFUMAP(text_l, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cluster Label', 'Cluster probabilities', 'Outlier Scores'], dtype='object')\n",
      "[7, 7, 7, 7, 7, 7, 7, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 13, 13, 12, 13, 13, 13, -1, 12, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 14, 14, 14, 15, 15, 15, 15, 15, 11, 11, 11, 11, 11, 11, 8, 8, 8, 8, 8, 4, 8, 3, 4, 4, 4, 4, 4, 4, -2, -3, 16, 16, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "df = clusterHDBSCANNoLabels(vectorized, printSummary= False)\n",
    "print(df.columns)\n",
    "print(list(df['Cluster Label']))\n",
    "\n",
    "clusterLabels = df['Cluster Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordsYake(clusterLabels):\n",
    "  clusterLabelsNumpy = np.array(clusterLabels)\n",
    "  seen = []\n",
    "  groups = [\"\"] * len(set(clusterLabels))\n",
    "  count = 0 \n",
    "  for x in range(len(clusterLabels)):\n",
    "    if(clusterLabels[x] not in seen):\n",
    "      seen.append(clusterLabels[x])\n",
    "      locations = np.where(clusterLabelsNumpy == clusterLabels[x])[0].tolist()\n",
    "      for x in (locations):\n",
    "        groups[count] = groups[count] + \" \" + text_l[x]\n",
    "      count = count + 1\n",
    "\n",
    "  keywords = []\n",
    "  for x in groups:\n",
    "    currentKeyword = kw_extractor.extract_keywords(x)\n",
    "    keywords.append(currentKeyword)\n",
    "  return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywordResultsClusterLDA(groupLabels(clusterLabels, text_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LDA and HDP\n",
    "def order_subset_by_coherence(dirichlet_model, bow_corpus, num_topics=10, num_keywords=10):\n",
    "    \"\"\"\n",
    "    Orders topics based on their average coherence across the corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dirichlet_model : gensim.models.type_of_model\n",
    "        bow_corpus : list of lists (contains (id, freq) tuples)\n",
    "        num_topics : int (default=10)\n",
    "        num_keywords : int (default=10)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        ordered_topics, ordered_topic_averages: list of lists and list\n",
    "    \"\"\"\n",
    "    if type(dirichlet_model) == LdaModel:\n",
    "        shown_topics = dirichlet_model.show_topics(num_topics=num_topics, \n",
    "                                                   num_words=num_keywords,\n",
    "                                                   formatted=False)\n",
    "    elif type(dirichlet_model)  == HdpModel:\n",
    "        shown_topics = dirichlet_model.show_topics(num_topics=150, # return all topics\n",
    "                                                   num_words=num_keywords,\n",
    "                                                   formatted=False)\n",
    "    model_topics = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "    topic_corpus = dirichlet_model.__getitem__(bow=bow_corpus, eps=0) # cutoff probability to 0 \n",
    "\n",
    "    topics_per_response = [response for response in topic_corpus]\n",
    "    flat_topic_coherences = [item for sublist in topics_per_response for item in sublist]\n",
    "\n",
    "    significant_topics = list(set([t_c[0] for t_c in flat_topic_coherences])) # those that appear\n",
    "    topic_averages = [sum([t_c[1] for t_c in flat_topic_coherences if t_c[0] == topic_num]) / len(bow_corpus) \\\n",
    "                      for topic_num in significant_topics]\n",
    "\n",
    "    topic_indexes_by_avg_coherence = [tup[0] for tup in sorted(enumerate(topic_averages), key=lambda i:i[1])[::-1]]\n",
    "\n",
    "    significant_topics_by_avg_coherence = [significant_topics[i] for i in topic_indexes_by_avg_coherence]\n",
    "    ordered_topics = [model_topics[i] for i in significant_topics_by_avg_coherence][:num_topics] # limit for HDP\n",
    "\n",
    "    ordered_topic_averages = [topic_averages[i] for i in topic_indexes_by_avg_coherence][:num_topics] # limit for HDP\n",
    "    ordered_topic_averages = [a/sum(ordered_topic_averages) for a in ordered_topic_averages] # normalize HDP values\n",
    "\n",
    "    return ordered_topics, ordered_topic_averages\n",
    "\n",
    "def getKeywordsLDA(group):\n",
    "\n",
    "    #print((group[0]).split())\n",
    "    \"\"\"for x in range(len(group)):\n",
    "        group[x] = group[x].split()\"\"\"\n",
    "    dirichlet_dict = corpora.Dictionary(group)\n",
    "    bow_corpus = [dirichlet_dict.doc2bow(text) for text in group]\n",
    "\n",
    "    num_topics = 10\n",
    "    num_keywords = 5\n",
    "\n",
    "    dirichlet_model = LdaModel(corpus=bow_corpus,\n",
    "                            id2word=dirichlet_dict,\n",
    "                            num_topics=num_topics,\n",
    "                            update_every=1,\n",
    "                            chunksize=len(bow_corpus),\n",
    "                            passes=20,\n",
    "                            alpha='auto')\n",
    "\n",
    "    ordered_topics, ordered_topic_averages = \\\n",
    "        order_subset_by_coherence(dirichlet_model=dirichlet_model,\n",
    "                                bow_corpus=bow_corpus, \n",
    "                                num_topics=num_topics,\n",
    "                                num_keywords=num_keywords)\n",
    "\n",
    "    keywords = []\n",
    "    for i in range(num_topics):\n",
    "        # Find the number of indexes to select, which can later be extended if the word has already been selected\n",
    "        selection_indexes = list(range(int(round(num_keywords * ordered_topic_averages[i]))))\n",
    "        if selection_indexes == [] and len(keywords) < num_keywords: \n",
    "            # Fix potential rounding error by giving this topic one selection\n",
    "            selection_indexes = [0]\n",
    "                \n",
    "        for s_i in selection_indexes:\n",
    "            if ordered_topics[i][s_i] not in keywords:\n",
    "                keywords.append(ordered_topics[i][s_i])\n",
    "            else:\n",
    "                selection_indexes.append(selection_indexes[-1] + 1)\n",
    "\n",
    "    # Fix for if too many were selected\n",
    "    keywords = keywords[:num_keywords]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def keywordResultsGroup(textGroups, func):\n",
    "    textLabel = list(textGroups[\"Labels\"])\n",
    "    textArticles = list(textGroups[\"Text groups\"])\n",
    "    allKeywords = []\n",
    "    for x in range(len(textLabel)):\n",
    "        #print(textArticles[x])\n",
    "        allKeywords.append(func(textArticles[x]))\n",
    "    \n",
    "    keywordDF = pd.DataFrame(data = {\"Topic\": textLabel, \"Keywords\": allKeywords})\n",
    "    return keywordDF\n",
    "\n",
    "print(keywordResultsGroup(textGroups, getKeywordsLDA).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(top = 5)\n",
    "keywords = kw_extractor.extract_keywords(text_l[15])\n",
    "print(keywords)\n",
    "print(text_l[1])\n",
    "#print(keywords)\n",
    "#print(getKeywordsYake(clusterLabels)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords\n",
    "\n",
    "clusterLabelsNumpy = np.array(clusterLabels)\n",
    "seen = []\n",
    "groups = [\"\"] * len(set(clusterLabels))\n",
    "count = 0 \n",
    "for x in range(len(clusterLabels)):\n",
    "    if(clusterLabels[x] not in seen):\n",
    "        seen.append(clusterLabels[x])\n",
    "        locations = np.where(clusterLabelsNumpy == clusterLabels[x])[0].tolist()\n",
    "        for x in (locations):\n",
    "            groups[count] = groups[count] + \" \" + rawText[x]\n",
    "        count = count + 1\n",
    "\n",
    "keywordList = []\n",
    "for x in groups:\n",
    "    TR_keywords = keywords.keywords(x, scores=True)\n",
    "    keywordList.append(TR_keywords[0:5])\n",
    "\n",
    "print(keywordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xmp', 0.3668069576946353), ('migrate', 0.3207280575858291), ('migrated', 0.3207280575858291), ('bird', 0.168915883542472), ('briedis etal', 0.16691740548376394)]\n"
     ]
    }
   ],
   "source": [
    "from summa import keywords\n",
    "TR_keywords = keywords.keywords(text_l[4], scores=True)\n",
    "print(TR_keywords[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "textList = \"\"\"Google quietly rolled out a new way for Android users to listen \n",
    "to podcasts and subscribe to shows they like, and it already works on \n",
    "your phone. Podcast production company Pacific Content got the exclusive \n",
    "on it.This text is taken from Google news.\"\"\"\n",
    "r.extract_keywords_from_text(textList)\n",
    "#r.get_ranked_phrases()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterLabelsNumpy = np.array(clusterLabels)\n",
    "seen = []\n",
    "groups = [\"\"] * len(set(clusterLabels))\n",
    "count = 0 \n",
    "for x in range(len(clusterLabels)):\n",
    "    if(clusterLabels[x] not in seen):\n",
    "        seen.append(clusterLabels[x])\n",
    "        locations = np.where(clusterLabelsNumpy == clusterLabels[x])[0].tolist()\n",
    "        for x in (locations):\n",
    "            groups[count] = groups[count] + \" \" + rawText[x]\n",
    "        count = count + 1\n",
    "\n",
    "keywordList = []\n",
    "#kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "kw_model = KeyBERT(model = 'all-MiniLM-L6-v2')\n",
    "for x in groups:\n",
    "    currentKeyword = kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 2), stop_words='english', highlight=False, top_n=5)\n",
    "    keywordList.append(list(dict(currentKeyword).keys()))\n",
    "    print(currentKeyword)\n",
    "print(keywordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "                                      Topic                                 Keywords\n",
      "0                                    Alloys           [metallurgy, aluminium, alloy]\n",
      "1                           Avian Migration        [migratory, migrating, migration]\n",
      "2                             Breash Cancer      [mammography, imaging, bcr_imaging]\n",
      "3                                      Cake                 [baked, dessert, recipe]\n",
      "4                           Circadian Rythm           [sleep, sleepiness, circadian]\n",
      "5                               Court Cases           [judicial, judge, segregation]\n",
      "6                            Food Allergies              [allergy, allergic, asthma]\n",
      "7                                 Fugatives              [fugitive, suspect, arrest]\n",
      "8                             Horror Movies       [cast, cinematography, screenplay]\n",
      "9                    Humanized Mice Testing  [hiv, immunodeficiency, antiretroviral]\n",
      "10  Intelligent Data Analytics and Learning               [acrobat, pdfafield, pdfx]\n",
      "11        Medical Ethics - Informed Consent      [consultation, clinical, procedure]\n",
      "12                                   Plants            [botanical, taxonomic, flora]\n",
      "13                                  Pokemon            [pokmon, pokemon, charmander]\n",
      "14                          Prostate Cancer          [prostate, acrobat, anticancer]\n",
      "15                           Sport Injuries          [injury, athlete, epidemiology]\n",
      "16                            US Presidents    [presidential, presidency, president]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keywords1 = kw_model.extract_keywords(docs = rawText[0], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\\nkeywords1List = list(itertools.chain.from_iterable(keywords1))\\nkeywords2 = kw_model.extract_keywords(docs = rawText[1], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\\nkeywords2List = list(itertools.chain.from_iterable(keywords2))\\nkeywords3 = kw_model.extract_keywords(docs = rawText[2], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\\nkeywords3List = list(itertools.chain.from_iterable(keywords3))\\ntotal = keywords1List + keywords2List + keywords3List\\ntotalWords = total[0::2]\\nmostCommon = Counter(totalWords)\\nmost_occur = mostCommon.most_common(3)'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "df[\"Labels\"] = labels\n",
    "df[\"Text\"] = text_l\n",
    "textGroups = df.groupby(\"Labels\")[\"Text\"].apply(list).reset_index(name='Text groups')\n",
    "\n",
    "#model=\"all-MiniLM-L6-v2\"\n",
    "#model='all-mpnet-base-v2'\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "textLabel = list(textGroups[\"Labels\"])\n",
    "textArticles = list(textGroups[\"Text groups\"])\n",
    "allKeywords = []\n",
    "\n",
    "def getKeywordsKeyBERT(articles):\n",
    "    total = []\n",
    "    \n",
    "    for x in articles:\n",
    "        #print(x)\n",
    "        keywordsTemp = kw_model.extract_keywords(docs = x, keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "        #print(keywordsTemp)\n",
    "        total = total + list(itertools.chain.from_iterable(keywordsTemp))\n",
    "    totalWords = total[0::2]\n",
    "    #mostCommon = Counter(totalWords)\n",
    "    #most_occur = mostCommon.most_common(3)\n",
    "    #print(totalWords)\n",
    "    most_common_words= [word for word, word_count in Counter(totalWords).most_common(3)]\n",
    "    #print(totalWords)\n",
    "    #print(most_common_words)\n",
    "    return most_common_words\n",
    "\n",
    "smallLabels = textLabel[0:2]\n",
    "#print(smallLabels)\n",
    "#print(len(smallLabels))\n",
    "for x in range(len(textLabel)):\n",
    "    print(x)\n",
    "    allKeywords.append(getKeywordsKeyBERT(textArticles[x]))\n",
    "\n",
    "keywordDF = pd.DataFrame(data = {\"Topic\": textLabel, \"Keywords\": allKeywords})\n",
    "\n",
    "print(keywordDF.to_string())\n",
    "\n",
    "\"\"\"keywords1 = kw_model.extract_keywords(docs = rawText[0], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords1List = list(itertools.chain.from_iterable(keywords1))\n",
    "keywords2 = kw_model.extract_keywords(docs = rawText[1], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords2List = list(itertools.chain.from_iterable(keywords2))\n",
    "keywords3 = kw_model.extract_keywords(docs = rawText[2], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords3List = list(itertools.chain.from_iterable(keywords3))\n",
    "total = keywords1List + keywords2List + keywords3List\n",
    "totalWords = total[0::2]\n",
    "mostCommon = Counter(totalWords)\n",
    "most_occur = mostCommon.most_common(3)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textGroups.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aluminium', 0.6596, 'aluminum', 0.6185, 'isotopes_of_aluminium', 0.6114, 'aluminium_alloy', 0.5896, 'aluminium_alloys', 0.5849, 'aluminium_oxide', 0.5802, 'aluminiumâ', 0.5726, 'alkali_metals', 0.5647, 'aluminium_arsenide', 0.564, 'native_aluminium', 0.5634, 'beryllium', 0.6361, 'beryllium_iron', 0.6233, 'berylliumâ', 0.6105, 'beryllium_copper', 0.599, 'beryllium_nitride', 0.5877, 'alkaline_earth_metal', 0.5729, 'alkali_metals', 0.5706, 'beryllium_oxide', 0.5686, 'beryllium_nitrate', 0.5664, 'beryllium_sulfate', 0.5657, 'isotopes_of_bismuth', 0.5462, 'bismuth_crystals', 0.5373, 'bismuth_oxychloride', 0.5115, 'bismuthyl', 0.5084, 'chemical_element', 0.4986, 'bismuthinite', 0.4859, 'bismoclite', 0.4843, 'bismuth_bronze', 0.4758, 'bismuth_oxide', 0.4705, 'bismuthine', 0.4696]\n",
      "['alkali_metals', 'aluminium', 'aluminum']\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "#model=\"all-MiniLM-L6-v2\"\n",
    "#model='all-mpnet-base-v2'\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "textLabel = list(textGroups[\"Labels\"])\n",
    "textArticles = list(textGroups[\"Text groups\"])\n",
    "allKeywords = []\n",
    "\n",
    "\"\"\"def getKeywordsKeyBERT(articles):\n",
    "    total = []\n",
    "    for x in articles:\n",
    "        keywordsTemp = kw_model.extract_keywords(docs = rawText[0], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "        total = total + keywordsTemp\n",
    "    totalWords = total[0::2]\n",
    "    mostCommon = Counter(totalWords)\n",
    "    most_occur = mostCommon.most_common(3)\n",
    "    return most_occur\n",
    "\n",
    "smallLabels = textLabel[0]\n",
    "print(len(smallLabels)\n",
    "\n",
    "for x in range(len(smallLabels)):\n",
    "    print(x)\n",
    "    allKeywords.append(getKeywordsKeyBERT(textArticles[x]))\n",
    "\n",
    "keywordDF = pd.DataFrame(data = {\"Topic\": textLabel[0], \"Keywords\": allKeywords})\n",
    "\n",
    "print(keywordDF)\"\"\"\n",
    "\n",
    "keywords1 = kw_model.extract_keywords(docs = rawText[0], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords1List = list(itertools.chain.from_iterable(keywords1))\n",
    "keywords2 = kw_model.extract_keywords(docs = rawText[1], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords2List = list(itertools.chain.from_iterable(keywords2))\n",
    "keywords3 = kw_model.extract_keywords(docs = rawText[2], keyphrase_ngram_range=(1, 1), highlight=False, top_n=10)\n",
    "keywords3List = list(itertools.chain.from_iterable(keywords3))\n",
    "total = keywords1List + keywords2List + keywords3List\n",
    "totalWords = total[0::2]\n",
    "mostCommon = Counter(totalWords)\n",
    "most_occur = mostCommon.most_common(3)\n",
    "most_common_words= [word for word, word_count in Counter(totalWords).most_common(3)]\n",
    "print(total)\n",
    "print(most_common_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc6a87218d93c3eb3aa29305b960fd7fc714f48d8ac50960275ef4f8856ba0fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
